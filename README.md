# SEG2102-DBMS-Comparison
# Database Query Optimization Performance Study

## Project Overview
This project evaluates **query optimization techniques** (mainly indexing strategies) within three DBMS models:
- **PostgreSQL** (relational, ACID)
- **MongoDB** (document store, flexible schema)
- **Apache Cassandra** (distributed, query-driven design)

The study measures performance using synthetic transaction datasets of **100k, 500k, and 1M rows/documents**, focusing on:
- Equality and range filtering
- Aggregation performance
- Sorting + limiting
- Index impact on scalability and latency

ðŸ“Œ *The goal is not to compare which database is best, but to compare how optimization techniques improve query performance within each model.*

---

## Repository Structure
Database-Optimization-Study/
â”‚
â”œâ”€â”€ postgresql/
â”‚ â”œâ”€â”€ schema.sql # Table and index creation scripts
â”‚ â”œâ”€â”€ queries.sql # Query workload (Q1â€“Q4)
â”‚ â””â”€â”€ results/ # Explain plans & timing outputs
â”‚
â”œâ”€â”€ mongodb/
â”‚ â”œâ”€â”€ schema_setup.js # Index creation scripts
â”‚ â”œâ”€â”€ queries.js # Query workload + aggregation pipelines
â”‚ â””â”€â”€ results/ # executionStats explain outputs
â”‚
â”œâ”€â”€ cassandra/
â”‚ â”œâ”€â”€ schema.cql # Query-driven table designs
â”‚ â”œâ”€â”€ queries.cql # Workload queries (partition, clustering, pre-agg)
â”‚ â””â”€â”€ results/ # Query timing outputs
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ generate_dataset.py # Python script used to generate 100k / 500k / 1M datasets
â”‚ â”œâ”€â”€ transactions_100k.csv
â”‚ â”œâ”€â”€ transactions_500k.csv
â”‚ â””â”€â”€ transactions_1M.csv
â”‚
â””â”€â”€ report/


---

## System & Tools Used
- **OS:** Windows 10
- **Hardware:**
  - RAM: 16GB
  - CPU: Intel i5 (11th Gen)
- **Databases:** PostgreSQL, MongoDB, Cassandra (running via Docker containers)
- **Scripting:** Python (dataset generation + query automation)
- **Version Control:** GitHub (for transparency and reproducibility)

---

